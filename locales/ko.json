{
  "home": {
    "lowerContent": {
      "p1": "SEESAW는 김현동, 박단비, 서승우, 최윤서로 구성된 AI 인터랙션\n콜렉티브로, 사운드·시각 디자인, AI 개발, HCI 연구를 기반으로\n인공지능 기술과 감각적 경험 사이의 번역 구조를 설계한다.",
      "p2": "데이터 추출과 인식 기술을 통해 감정, 행동, 리듬과 같은 인간의 신호를\n감지하고, 이를 시각과 사운드의 언어로 재구성하며 기술은 단순한\n구현의 대상이 아니라, 사회적 구조를 드러내는 매개로 여긴다.",
      "p3": "우리는 알고리즘이 인간을 해석하는 방식, 그리고 그 해석이 사회적\n규범과 권력, 감각의 형식에 어떤 영향을 미치는지를 경험의 형태로\n제안하고자 한다.",
      "p4": "AI는 가능성이자 역설이다.\n확장과 연결을 증폭시키는 동시에, 표준화와 통제를 내포한다.\nSEESAW는 이 양면성을 하나의 스펙트럼으로 인식하며,\n기술의 순기능과 구조적 긴장을 동시에 탐구한다.",
      "p5": "판단을 제시하기보다 질문을 생성하고,\n결론을 단정하기보다 담론을 열어두는 방식으로",
      "p6": "SEESAW는 기술과 인간, 데이터와 감정,\n시스템과 사회가 교차하는 지점에서 발생하는 균형의 움직임을\n관찰하며, 그 긴장 속에서 새로운 감각적 언어와\n기술 담론을 지속적으로 실험한다."
    }
  },
  "works": {
    "resonanceProtocol": {
      "p1": "《Resonance Protocol》은 인간의 감정이 기술에 의해 '읽히는' 순간을 다루는 대신, 그 감정이 어떻게 형식화되고 배열되는지를 드러내는 VR 작업이다. 관객은 어두운 가상 공간에 입장하며, 자신의 호흡과 음성, 움직임이 빛과 사운드의 파동으로 변환되는 경험을 한다. 이 번역은 직관적이고 아름답게 느껴지지만, 시간이 지날수록 그 구조는 점차 특정한 스펙트럼으로 정렬된다.",
      "p2": "감정은 자유로운 흐름이 아니라, 점점 안정적이고 예측 가능한 범주로 재구성된다. 관객은 표현하고 있다고 느끼는 동시에, 더 선명하고 조화로운 피드백을 얻기 위해 스스로를 조정하고 있음을 인식하게 된다. 이 과정에서 기술은 단순한 감지 장치를 넘어 감정의 형식을 설계하는 체계로 작동한다.",
      "p3": "SEESAW는 이 작업을 통해 감정 인식 기술이 중립적인 도구가 아니라, 인간의 감각을 일정한 구조 안으로 배치하는 번역 장치임을 제안한다. 《Resonance Protocol》은 질문을 남긴다. 우리가 경험하는 감정은 얼마나 자율적인가. 그리고 기술이 제시하는 '안정된 모델'은 누구의 기준 위에 세워져 있는가."
    },
    "signalAsForm": {
      "overview": {
        "title": "Sound Visualization",
        "intro1": "Sound Visualization은 인공지능을 감각을 확장하는 번역 장치로 다루는 연구에서 출발하며 본 작업은 소리를 청각적 현상이 아닌, 분석 가능한 신호 구조로 인식하고 그 물리적 특성을 시각적·공간적 형태로 전환하는 과정을 탐구한다.",
        "intro2": "데이터 추출 및 인식 기술을 통해 음량, 진폭, 주파수, 리듬을 분해하고, 이를 파동, 점, 입자, 스펙트럼의 네 가지 표현 방식으로 재구성하였다. 시간에 따라 변화하는 음량은 입체 곡선으로, 주파수 분포는 점으로 이루어진 구체로, 리듬은 입자의 확산과 밀도 변화로 번역된다. 이 시각화는 소리를 형태와 움직임으로 경험하게 하며, 청각 중심 감각을 시각적 구조로 확장하는 감각 전환의 실험이다.",
        "spec": "2024, Audio-visual Research Prototype, Real-time generative visualization, Stereo Sound, Dimensions variable",
        "technicalTitle": "Technical Setup",
        "technicalBody": "Stereo sound system, Full HD projection, Real-time processing workstation, Microphone input",
        "softwareTitle": "Software"
      },
      "ai": {
        "intro": "본 프레임워크는 인공지능을 감각 번역 시스템으로 다루며,\n음향 신호를 정서 밀도로 전환하는 과정을 탐구한다.",
        "impactTitle": "Our AI Impact",
        "blocks": [
          {
            "title": "음색 분석 기반 PBR 머티리얼 매핑 AI",
            "subtitle": "Timbre Analysis-Based Physically Based Rendering (PBR) Material Mapping AI",
            "body": [
              "같은 음이라도 금속처럼 차갑게, 나무처럼 따뜻하게 들릴 수 있다. 음색은 단순한 음높이가 아니라, 구조이다.",
              "AI는 주파수 스펙트럼, 배음 밀도, 에너지 분포 등 음향의 물리적 특성을 추출하여 이 데이터는 Physically Based Rendering(PBR) 기법을 통해 표면 질감의 속성으로 번역된다.",
              "소리는 표면이 되고, 주파수는 질감이 된다.",
              "하나의 음은 반사율이 높은 금속처럼 시각화되거나, 따뜻한 목재 질감으로 나타날 수 있다. 이는 은유가 아니라, 데이터 기반 번역이다.",
              "이 과정은 음악의 질적 차이를 시각적·공간적 구조로 드러내며, 음색 인지의 접근성을 확장한다."
            ]
          },
          {
            "title": "Valence-Arousal 추정 기반 컬러 매핑 AI",
            "subtitle": "Valence-Arousal Estimation-Based Color Mapping AI",
            "body": [
              "음악 감정 연구에서는 Russell의 Circumplex Model of Affect가 널리 활용된다. Valence(쾌·불쾌)와 Arousal(긴장·활동성)은 정서를 구성하는 이차원 구조이다.",
              "저희 AI는 멜스펙트로그램, 템포, 리듬, 크로마 등의 특징을 분석하여 실시간으로 정서 벡터를 추정하고, 이 벡터는 색 공간으로 매핑된다.",
              "높은 각성과 긍정적 정서는 따뜻한 색조로, 낮은 각성이나 차분한 정서는 차가운 색조로 변환된다.",
              "청자는 감정을 해석하는 대신, 그 흐름을 색과 공간으로 경험할 수 있도록 하는데 목표로 한다."
            ]
          },
          {
            "title": "MR 기반 시각·햅틱 피드백 시스템",
            "subtitle": "MR-Based Visual and Haptic Feedback System",
            "body": [
              "소리는 본래 공기의 진동이다.",
              "MR 환경에서는 이 진동을 빛, 색, 촉각 진동으로 전환할 수 있다. 해당 시스템은 실시간 음향 분석 데이터를 MR 시각화와 햅틱 인터페이스에 통합한다.",
              "파동은 시각적 필드로 드러나고, 리듬은 신체적 진동으로 전달된다. 이 구조는 농인과 비농인이 서로 다른 감각을 통해 동일한 소리를 경험하도록 설계되었다.",
              "이는 청각의 대체가 아니라, 지각의 재분배이며, 다감각적 접근성의 확장으로 접근하고자 한다."
            ]
          }
        ]
      }
    },
    "safeRangeSystem": {
      "ai": {
        "description": [
          "SAFE RANGE SYSTEM은 실시간 얼굴 인식과 음성 분석 기술을 기반으로 작동하는 알고리즘 시스템이다.",
          "카메라와 마이크를 통해 수집된 데이터는 안면 특징점 추출, 미세 표정 분석, 시선 움직임 추적, 음성 스펙트럼 분석 및 발화 간 간격 측정 과정을 거쳐 정량화된다. 이 데이터는 통계적 평균과의 편차를 기준으로 파라미터화되고, 정규화 알고리즘을 통해 허용 범위 안으로 재배치된다.",
          "얼굴은 평균 모델에 기반한 3D 마스크로 출력되며, 음성은 디지털 신호 처리와 BPM 동기화 과정을 통해 74 BPM 리듬 구조에 정렬된다. 결과적으로 개인의 고유한 신호는 확장되는 대신, 통계적 안정성을 중심으로 재정렬되며, 알고리즘은 신호를 표준으로 수렴시키는 메커니즘으로 기능한다."
        ],
        "row2": [
          "본 작품은 관객의 얼굴에서 감지되는 미세한 신호들을 입력값으로 수집하는 시스템으로 구성된다. 눈의 움직임, 동공의 반응, 입술의 곡선 변화, 안면 근육의 긴장도와 같은 실시간 시각 데이터는 라이브 입력 단계에서 개별 요소로 분리되어 측정되며, 이후 알고리즘적 해석(Algorithmic Interpretation) 단계로 전달된다.",
          "이 단계에서 각 신호는 'Divergence Index'라는 값으로 변환되어 통계적 평균으로부터 얼마나 벗어나 있는지가 계산된다. 이러한 편차는 개인의 고유한 특성으로 유지되지 않고, 파라미터화 과정을 거쳐 수치화된 구조적 값으로 다시 배열된다.",
          "정리된 데이터는 정규화 필드를 통과하면서 허용 가능한 범위 안으로 조정된다. 이후 평균 모델을 기준으로 재구성되어, 최종적으로 3D 마스크 형태로 시각화된다.",
          "이 시스템은 새로운 얼굴을 창조하는 것이 아니라, 측정된 차이를 표준 구조에 맞추어 조정하는 방식으로 작동한다."
        ]
      },
      "exhibition": {
        "body": "본 작품은 개별 참여형 인터랙티브 설치로 구성되어, 관객의 얼굴과 목소리가 실시간으로 분석되고 재구성되는 과정을 현장에서 직접 경험할 수 있도록 제시되었다. 관객은 짧은 시간 동안 알고리즘이 표정과 발화를 정렬하는 과정을 체험하며, 스크린에는 평균 모델과 동기화된 결과가 시각적으로 드러난다. 작품은 기술적 처리 과정을 노출함으로써, 개인의 신호가 어떻게 평균 구조로 수렴되는지를 공간 안에서 체감하도록 구성되었다."
      }
    }
  }
}
